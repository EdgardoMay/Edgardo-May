# ğŸŒ ETL Pipeline con Airflow, MongoDB y Streamlit

Este proyecto implementa un pipeline ETL completo, construido para la asignatura de **GestiÃ³n Masiva de Datos** de la Universidad PolitÃ©cnica de YucatÃ¡n. Se utilizan tres APIs pÃºblicas que proporcionan datos del clima, noticias y documentos legales. El flujo se orquesta con Apache Airflow, se almacena en MongoDB y se visualiza a travÃ©s de un dashboard interactivo con Streamlit.

---

## ğŸ¯ Objetivo

DiseÃ±ar un sistema ETL batch que integre mÃºltiples fuentes de datos pÃºblicas mediante tÃ©cnicas de extracciÃ³n, transformaciÃ³n y carga, todo orquestado con Airflow, almacenado en MongoDB y visualizado grÃ¡ficamente en tiempo real con Streamlit.

---

## ğŸ§° TecnologÃ­as Utilizadas

| Componente      | TecnologÃ­a       | Puerto | DescripciÃ³n                                              |
|-----------------|------------------|--------|----------------------------------------------------------|
| Orquestador     | Apache Airflow   | 8080   | Orquesta tareas ETL y visualiza DAGs                    |
| Base de datos   | PostgreSQL       | N/A    | AlmacÃ©n de metadatos de Airflow                         |
| Almacenamiento  | MongoDB          | 27017  | Guarda datos crudos y transformados                     |
| Dashboard       | Streamlit        | 8501   | Interfaz web para visualizar los datos procesados       |

---

## ğŸŒ APIs Involucradas

1. **Open-Meteo API (Clima)**
   - Datos horarios: temperatura a distintas alturas, probabilidad de precipitaciÃ³n.
   - Requiere ubicaciÃ³n (lat/lon). Sin autenticaciÃ³n.

2. **Federal Register API**
   - Documentos oficiales publicados por el gobierno de EE.UU.
   - Permite filtros por tipo y fecha. No requiere API key.

3. **NewsAPI**
   - Titulares recientes de noticias. Requiere API Key gratuita.

---

## ğŸ“ Estructura del Proyecto

```plaintext
project/
â”œâ”€â”€ airflow/
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ dags/
â”‚       â”œâ”€â”€ ingestion_air_quality.py
â”‚       â”œâ”€â”€ ingestion_federal_register.py
â”‚       â”œâ”€â”€ ingestion_newsapi.py
â”‚       â”œâ”€â”€ transform_air_quality.py
â”‚       â”œâ”€â”€ transform_federal_register.py
â”‚       â”œâ”€â”€ transform_newsapi.py
â”‚       â”œâ”€â”€ load_mongo.py
â”‚       â””â”€â”€ pipeline.py
â”œâ”€â”€ streamlit_app/
â”‚   â”œâ”€â”€ app.py
â”‚   â””â”€â”€ Dockerfile
â”œâ”€â”€ utils/
â”‚   â””â”€â”€ mongo_utils.py
â”œâ”€â”€ .env
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ setup_pipeline.sh
â”œâ”€â”€ teardown_pipeline.sh
â””â”€â”€ README.md
```

---

## ğŸš€ EjecuciÃ³n del Proyecto

### OpciÃ³n A: Usando Script Automatizado

1. **Pre-requisitos**  
   Tener instalado y corriendo Docker Desktop.

2. **Dar permisos de ejecuciÃ³n al script (solo la primera vez):**
```bash
chmod +x setup_pipeline.sh
```

3. **Ejecutar el script:**

- OpciÃ³n 1:

```bash
./setup_pipeline.sh
```

- OpciÃ³n 2:

```bash
bash setup_pipeline.sh
```

Este script:
- Inicializa Airflow.
- Crea un usuario administrador.
- Levanta todos los servicios vÃ­a `docker-compose`.

4. **Acceder a las interfaces:**
   - Airflow: [http://localhost:8080](http://localhost:8080)
   - Streamlit: [http://localhost:8501](http://localhost:8501)

---

### OpciÃ³n B: ConfiguraciÃ³n Manual Paso a Paso

1. **Inicializar la base de datos de Airflow (solo la primera vez):**

```bash
docker compose run airflow-webserver airflow db init
```

2. **Crear usuario administrador en Airflow:**

```bash
docker compose run airflow-webserver airflow users create   --username admin   --firstname Edgardo   --lastname May   --role Admin   --email edgar@example.com   --password airflow
```

3. **Levantar todos los servicios:**

```bash
docker compose up -d --build
```

4. **Verificar que los contenedores estÃ©n activos:**

```bash
docker compose ps
```

---

## ğŸ“ˆ VisualizaciÃ³n de Datos

1. **Activar el DAG principal:**
   - Ir a [http://localhost:8080](http://localhost:8080)
   - Activar el DAG `main_etl_pipeline`.
   - Ejecutarlo manualmente desde la UI (botÃ³n â–¶ï¸).

2. **Esperar a que todas las tareas se completen (color verde).**

3. **Ir al dashboard de Streamlit:**
   - [http://localhost:8501](http://localhost:8501)
   - Se mostrarÃ¡n visualizaciones para:
     - ğŸŒ¤ï¸ Clima por hora (temperaturas, precipitaciones).
     - ğŸ“š Documentos federales recientes.
     - ğŸ—ï¸ Noticias recientes por fuente y fecha.

---

## ğŸ§¹ Limpieza del Entorno

Para detener y eliminar todos los contenedores y redes:

```bash
docker compose down
```

Para eliminar tambiÃ©n los volÃºmenes (datos de MongoDB y Airflow):

```bash
docker compose down -v
```

---

## âœ… Notas Finales

- Las credenciales de Airflow y las claves API estÃ¡n definidas en el archivo `.env`.
- Puedes modificar los DAGs o agregar nuevos sin reiniciar todos los servicios, solo reiniciar Airflow si agregas nuevos scripts.
- El archivo `teardown_pipeline.sh` elimina el entorno por completo.

---

## ğŸ“¬ Contacto

Proyecto desarrollado por **Edgardo May** como parte del curso de IngenierÃ­a de Datos, Universidad PolitÃ©cnica de YucatÃ¡n.
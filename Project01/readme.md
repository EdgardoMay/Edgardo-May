# ETL Project

This project performs ETL on data from 3 public APIs: Air Quality, News, and Federal Register.

1. Clone the repository:
   ```bash
   git clone <your_repo>
   cd <your_repo>

2. Build the containers:
    docker-compose up --build

3. Access to Airflow:
    http://localhost:8080
    user: admin
    pass: admin

4. Trigger DAGs:
    etl_airquality_dag
    etl_news_dag
    etl_fedregister_dag

5. Output:
    Cleaned data is loaded into MongoDB
    JSON backup can be dumped with:
        docker exec -it mongodb bash
        mongodump --out /dump



# Proyecto ETL con Apache Airflow y MongoDB

Este proyecto implementa un pipeline ETL (ExtracciÃ³n, TransformaciÃ³n y Carga) para recopilar datos desde tres APIs pÃºblicas (calidad del aire, noticias y documentos del Federal Register), procesarlos y almacenarlos en una base de datos MongoDB. El flujo estÃ¡ orquestado mediante Apache Airflow y empaquetado con Docker.

---

## ğŸš€ TecnologÃ­as utilizadas

- **Apache Airflow 2.8.1** â€“ OrquestaciÃ³n del pipeline
- **MongoDB 6.0** â€“ Almacenamiento NoSQL
- **Python 3.10** â€“ Scripts ETL modulares
- **Docker & Docker Compose** â€“ Contenedores y gestiÃ³n del entorno

---

## ğŸ“ Estructura del proyecto

```
project-root/
â”œâ”€â”€ dags/                   # DAGs de Airflow (opcional)
â”œâ”€â”€ ingestion/              # Scripts de extracciÃ³n
â”œâ”€â”€ transform/              # Scripts de transformaciÃ³n
â”œâ”€â”€ mainpipeline.py         # Ejecuta el pipeline completo
â”œâ”€â”€ loadmongo.py            # Carga datos a MongoDB
â”œâ”€â”€ requirements.txt        # Dependencias Python
â”œâ”€â”€ docker-compose.yml      # OrquestaciÃ³n de servicios
â”œâ”€â”€ dockerfile              # Imagen de Airflow personalizada
â”œâ”€â”€ .env                    # Variables de entorno (API keys, URI Mongo, etc.)
â”œâ”€â”€ setup_airflow.sh        # Script automÃ¡tico de instalaciÃ³n y arranque
â””â”€â”€ README.md               # DocumentaciÃ³n del proyecto
```

---

## âš™ï¸ InstalaciÃ³n y ejecuciÃ³n rÃ¡pida

### 1. Clona el repositorio y accede al directorio:
```bash
git clone https://github.com/usuario/proyecto-etl.git
cd proyecto-etl
```

### 2. Configura el archivo `.env`:

```dotenv
MONGO_URI=mongodb://mongodb:27017/
AIRFLOW__CORE__LOAD_EXAMPLES=False
```

### 3. Ejecuta el script automÃ¡tico:
```bash
./setup_airflow.sh
```

Esto harÃ¡:
- Inicializar la base de datos de Airflow
- Crear el usuario `airflow` / `airflow`
- Levantar todos los contenedores

### 4. Accede a la interfaz web:

ğŸ“ http://localhost:8080  
ğŸ‘¤ Usuario: `airflow`  
ğŸ”‘ ContraseÃ±a: `airflow`

---

## ğŸ§ª EjecuciÃ³n manual del pipeline ETL

Para ejecutar el pipeline completo de forma manual:
```bash
docker compose exec airflow python /app/mainpipeline.py
```

Esto extraerÃ¡ datos, los transformarÃ¡, los cargarÃ¡ en MongoDB y generarÃ¡ archivos `.json` de respaldo en tu mÃ¡quina host.

---

## ğŸ“ Notas importantes

- MongoDB se expone en el puerto `27017`
- Airflow usa SQLite por defecto (persistido en el volumen `airflow_data`)
- Los DAGs pueden ser migrados a Airflow posteriormente, si lo deseas
- Recuerda instalar Docker y Docker Compose antes de iniciar

---

## ğŸ‘¨â€ğŸ’» Autor

- **Edgardo May** â€“ IngenierÃ­a en Datos, Proyecto Escolar UPPY


